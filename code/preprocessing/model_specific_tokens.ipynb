{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c728999f",
   "metadata": {},
   "source": [
    "## BERT Specific Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a868b",
   "metadata": {},
   "source": [
    "These are:\n",
    "\n",
    "* Token\tMeaning\n",
    "* `[PAD]`:\tPadding token, allows us to maintain same-length sequences (512 tokens for Bert) even when different sized sentences are fed in\n",
    "* `[UNK]`:\tUsed when a word is unknown to Bert\n",
    "* `[CLS]`:\tAppears at the start of every sequence\n",
    "* `[SEP]`:\tIndicates a seperator or end of sequence\n",
    "* `[MASK]`:\tUsed when masking tokens, for example in training with masked language modelling (MLM)\n",
    "\n",
    "- So if we take the 'NLP models' tweet, processing that directly with our BERT specific tokens might look like this:\n",
    "\n",
    "['[CLS]', '[UNK]', 'thinks', 'that', 'the', 'nlp', 'models', 'that', '[UNK]', 'made', 'are', 'super', 'cool', '[SEP]', '[PAD]', '[PAD]', ..., '[PAD]']\n",
    "Here, we have:\n",
    "\n",
    "Applied [CLS] token to indicate the start of the sequence.\n",
    "Both username tokens @elonmusk and @joebloggs were not 'known' words to BERT so BERT replaced them with unknown tokens [UNK], alternatively we could have replaced these with our own special user tokens.\n",
    "Added *[SEP] token to the end of our sequence.\n",
    "Padded the sequence upto the required length of 512 tokens (required due to fixed input sequence length of BERT model) using [PAD] tokens.\n",
    "Different models will have different special tokens, but we will often that they are being used for similiar reasons.\n",
    "\n",
    "That's everything on tokens for now, although we will cover tokenization in more depth (and the code too) for different models in later notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c37959f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '[UNK]',\n",
       " 'thinks',\n",
       " 'that',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'models',\n",
       " 'that',\n",
       " '[UNK]',\n",
       " 'made',\n",
       " 'are',\n",
       " 'super',\n",
       " 'cool',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"[CLS] [UNK] thinks that the NLP models that [UNK] made are super cool [SEP] [PAD] [PAD]\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c82b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
